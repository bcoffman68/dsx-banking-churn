{
    "metadata": {
        "language_info": {
            "name": "scala", 
            "mimetype": "text/x-scala", 
            "pygments_lexer": "scala", 
            "version": "2.11.8", 
            "codemirror_mode": "text/x-scala", 
            "file_extension": ".scala"
        }, 
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.0", 
            "name": "scala-spark20", 
            "language": "scala"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "<img src=\"http://milepro.com/wp-content/uploads/2014/01/Travel-Credit-Cards-1024x606.jpg\" style=\"width:200px; float: left; padding-right: 10px\"/>\n<h2 style=\"font-face: verdana; font-size: 32px;\">Predict credit card customer churn<br>with IBM Watson Machine Learning</h2>\n<h3 style=\"font-face: verdana; font-size: 16px;\">Part 2: Deploy Churn Model</h3>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n### 1. Load the data into a dataframe ##\n-------------------------------------\n<p>In this section you will load the data as an Apache\u00ae Spark DataFrame and perform a basic exploration.</p>\n<p>Load the data to the Spark DataFrame by using wget to upload the data to gpfs and then read method.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "import org.apache.spark.SparkContext\nimport org.apache.spark.SparkConf\n\nsc.stop()\nval conf1 = new SparkConf().setAppName(\"spark_context\").setMaster(\"local[*]\")\nval scl = new SparkContext(conf1)", 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "<div class=\"alert alert-block alert-info\"> Note: Only run the cell above when you run this notebook the first time after you create it, or whenever you restart the kernel. If there is error about \"Only one SparkContext may be running in this JVM\", that is expected.</div> ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "source": "%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.3.jar -f", 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Starting download from https://brunelvis.org/jar/spark-kernel-brunel-all-2.3.jar\nFinished download of spark-kernel-brunel-all-2.3.jar\n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "import org.apache.spark.{SparkConf, SparkContext, SparkFiles}\nimport org.apache.spark.sql.{SQLContext, SparkSession, Row}\nimport org.apache.spark.SparkFiles\n\nimport org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer, VectorAssembler}\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.classification.{LogisticRegression, DecisionTreeClassifier}\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.{Pipeline, PipelineStage}\nimport org.apache.spark.ml.ibm.transformers.RenameColumn\n\nimport com.ibm.analytics.ngp.ingest.Sampling\nimport com.ibm.analytics.ngp.repository._\nimport com.ibm.analytics.ngp.util._\nimport com.ibm.analytics.ngp.pipeline.evaluate.{Evaluator,MLProblemType}\n\nimport com.ibm.analytics.wml.{Learner, Target}\nimport com.ibm.analytics.wml.cads.CADSEstimator", 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "<p>The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 4, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "<div class=\"alert alert-block alert-info\"> Note: When creating the project context, use the local spark context (scl) created above instead of the default spark context (sc).</div> \nGenerate a token using the Insert Token option (click vertical ellipses button), hten replace the credentials in the cell above)", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "\n\n### 1.1 Load TEST_SUM.csv from IBM Bluemix Object Store ###", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+----------+---+---+---------+----------+-------+--------+-----+--------+------------+-------+----------+---------+-----+----------------+---------+-----------+--------------------+\n|   CUST_ID|SEX|AGE|EDUCATION|INVESTMENT| INCOME|ACTIVITY|CHURN|YRLY_AMT|AVG_DAILY_TX|YRLY_TX|AVG_TX_AMT|NEGTWEETS|STATE| EDUCATION_GROUP|TwitterID|CHURN_LABEL|         INSERT_TIME|\n+----------+---+---+---------+----------+-------+--------+-----+--------+------------+-------+----------+---------+-----+----------------+---------+-----------+--------------------+\n|1009530860|  F| 84|        2|    114368|3852862|       5|    0|700259.0|    0.917808|    335|   2090.32|        3|   TX|Bachelors degree|        0|      false|2017-02-09 11:00:...|\n|1009544000|  F| 44|        2|     90298|3849843|       1|    0|726977.0|    0.950685|    347|   2095.04|       10|   CA|Bachelors degree|        0|      false|2017-02-09 11:00:...|\n|1009534260|  F| 23|        2|     94881|3217364|       1|    1|579084.0|    0.920548|    336|   1723.46|        5|   CA|Bachelors degree|        0|       true|2017-02-09 11:00:...|\n|1009574010|  F| 24|        2|    112099|2438218|       4|    1|470964.0|    0.994521|    363| 1297.4199|        2|   WA|Bachelors degree|        0|       true|2017-02-09 11:00:...|\n|1009578620|  F| 67|        5|     84638|2428245|       3|    0|446615.0|    0.917808|    335| 1333.1799|       10|   CT|       Doctorate|        0|      false|2017-02-09 11:00:...|\n+----------+---+---+---------+----------+-------+--------+-----+--------+------------+-------+----------+---------+-----+----------------+---------+-----------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "\n\n### 1.2 Select Churn Data for the Model ###\n<p>Select AGE, ACTIVITY, EDUCATION, SEX, STATE, NEGTWEETS, INCOME, CHURN from the churnDataRaw dataframe.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._ \n\nval toDouble = udf {x: Int => x.toDouble}\n\nval churnData = churnDataRaw.select(\"AGE\", \"ACTIVITY\", \"EDUCATION\", \"SEX\", \"STATE\", \"NEGTWEETS\", \"INCOME\", \"CHURN\").\n                             withColumn(\"label\", toDouble(churnDataRaw.col(\"CHURN\"))).\n                             drop(\"CHURN\")\nchurnData.show(5)", 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+--------+---------+---+-----+---------+-------+-----+\n|AGE|ACTIVITY|EDUCATION|SEX|STATE|NEGTWEETS| INCOME|label|\n+---+--------+---------+---+-----+---------+-------+-----+\n| 84|       5|        2|  F|   TX|        3|3852862|  0.0|\n| 44|       1|        2|  F|   CA|       10|3849843|  0.0|\n| 23|       1|        2|  F|   CA|        5|3217364|  1.0|\n| 24|       4|        2|  F|   WA|        2|2438218|  1.0|\n| 67|       3|        5|  F|   CT|       10|2428245|  0.0|\n+---+--------+---------+---+-----+---------+-------+-----+\nonly showing top 5 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n## 2. Create an Apache Spark machine learning model ##\n-------------------------------------\n<p>Prepare data, create an Apache Spark machine learning pipeline, and train a model.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n### 2.1 Prepare the Data ###\n<p>In this subsection you will split your data into: train, test and predict datasets.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "val train = 70\nval test = 15\nval validate = 15\n\nval splits = Sampling.trainingSplit(churnData, train, test, validate)\n\nval trainingDF = splits._1\nval testDF = splits._2\nval validationDF = splits._3\n\nprintln(\"Training data set\")\ntrainingDF.show(5)\n\nprintln(\"Testing data set\")\ntestDF.show(5)\n\nprintln(\"Validation data set\")\nvalidationDF.show(5)", 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Training data set\n+---+--------+---------+---+-----+---------+------+-----+\n|AGE|ACTIVITY|EDUCATION|SEX|STATE|NEGTWEETS|INCOME|label|\n+---+--------+---------+---+-----+---------+------+-----+\n| 20|       0|        1|  F|   CA|        7| 17088|  1.0|\n| 20|       0|        1|  F|   WA|       15| 15497|  1.0|\n| 20|       0|        1|  M|   CA|        7| 16982|  1.0|\n| 20|       1|        1|  F|   ID|        7| 19761|  1.0|\n| 20|       1|        1|  F|   PA|        6| 19556|  1.0|\n+---+--------+---------+---+-----+---------+------+-----+\nonly showing top 5 rows\n\nTesting data set\n+---+--------+---------+---+-----+---------+------+-----+\n|AGE|ACTIVITY|EDUCATION|SEX|STATE|NEGTWEETS|INCOME|label|\n+---+--------+---------+---+-----+---------+------+-----+\n| 20|       0|        1|  F|   ID|       13| 17877|  1.0|\n| 20|       1|        1|  M|   WV|        4| 20614|  0.0|\n| 20|       2|        1|  F|   KY|        3| 14811|  0.0|\n| 20|       2|        2|  M|   MI|       10| 43853|  1.0|\n| 20|       2|        4|  M|   PA|        5| 17511|  1.0|\n+---+--------+---------+---+-----+---------+------+-----+\nonly showing top 5 rows\n\nValidation data set\n+---+--------+---------+---+-----+---------+------+-----+\n|AGE|ACTIVITY|EDUCATION|SEX|STATE|NEGTWEETS|INCOME|label|\n+---+--------+---------+---+-----+---------+------+-----+\n| 20|       1|        1|  M|   ID|        6| 18453|  1.0|\n| 20|       1|        1|  M|   ND|       13| 16552|  1.0|\n| 20|       1|        1|  M|   PA|        1| 22424|  0.0|\n| 20|       2|        2|  M|   CA|        6| 25513|  1.0|\n| 20|       4|        2|  M|   MI|       10| 31195|  1.0|\n+---+--------+---------+---+-----+---------+------+-----+\nonly showing top 5 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n### 2.2 Create pipeline and train a model ###\n<p>In this section you will create an Apache\u00ae Spark machine learning pipeline and then train the model.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "//Feature definition\nval genderIndexer = new StringIndexer().setInputCol(\"SEX\").setOutputCol(\"gender_code\")\nval stateIndexer = new StringIndexer().setInputCol(\"STATE\").setOutputCol(\"state_code\")\nval featuresAssembler = new VectorAssembler().setInputCols(Array(\"AGE\", \n                                                         \"ACTIVITY\", \n                                                         \"EDUCATION\", \n                                                         \"NEGTWEETS\", \n                                                         \"INCOME\",\n                                                         \"gender_code\",\n                                                         \"state_code\")).setOutputCol(\"features\")", 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "<p>Next, define estimators you want to use for classification. Logistics Regression is used in the following example.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "val lr = new LogisticRegression().setRegParam(0.01).setLabelCol(\"label\").setFeaturesCol(\"features\")\nval decisionTree = new DecisionTreeClassifier().setMaxBins(50).setLabelCol(\"label\").setFeaturesCol(\"features\")\n", 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "<p>Setup a Cognitive Assistant for Data Scientists - predict model performance based on sampled data</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "val learners = List(Learner(\"LR\", lr), Learner(\"DT\", decisionTree))\nval cads = CADSEstimator().setEvaluator(new BinaryClassificationEvaluator().\n                           setMetricName(\"areaUnderROC\")).\n                           setLearners(learners).\n                           setKeepBestNLearnersParam(3).\n                           setTarget(Target(\"rawPrediction\", \"label\")).\n                           setNumSampleFoldsParam(2)\nval pipeline = new Pipeline().setStages(Array(genderIndexer, stateIndexer, featuresAssembler, cads))\nval model = pipeline.fit(trainingDF)\n", 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "<p>You can check your model accuracy now. To evaluate the model, use test data.</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "val predictions = model.transform(testDF)\nval evaluatorRF = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\nval accuracy = evaluatorRF.evaluate(predictions)\nprintln(\"Accuracy = \" + accuracy)\nprintln(\"Test Error = \" + (1.0 - accuracy))", 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Accuracy = 0.9276315789473685\nTest Error = 0.07236842105263153\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "import com.ibm.analytics.ngp.pipeline.evaluate._\nimport com.ibm.analytics.ngp.pipeline.evaluate.JsonMetricsModel._\nimport spray.json._\n\nval metrics = Evaluator.evaluateModel(MLProblemType.BinaryClassifier,model,testDF)\n\nprintln(s\"Binary Metric: ${metrics.asInstanceOf[BinaryClassificationMetricsModel].toJson}\")", 
            "execution_count": 12, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Binary Metric: {\"recallByThreshold\":[{\"threshold\":1.0,\"metric\":0.8467741935483871},{\"threshold\":0.0,\"metric\":1.0}],\"precisionByThreshold\":[{\"threshold\":1.0,\"metric\":0.8823529411764706},{\"threshold\":0.0,\"metric\":0.2719298245614035}],\"areaUnderPR\":0.8853969006957622,\"fMeasureByThreshold\":[{\"threshold\":1.0,\"metric\":0.8641975308641976},{\"threshold\":0.0,\"metric\":0.42758620689655175}],\"roc\":[{\"threshold\":0.0,\"metric\":0.0},{\"threshold\":0.04216867469879518,\"metric\":0.8467741935483871},{\"threshold\":1.0,\"metric\":1.0},{\"threshold\":1.0,\"metric\":1.0}],\"areaUnderROC\":0.902302759424796}\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "println(metrics.asInstanceOf[BinaryClassificationMetricsModel].roc)", 
            "execution_count": 13, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[Lcom.ibm.analytics.ngp.pipeline.evaluate.ThresholdMetricModel;@1bea81e9\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "\n<p>Create a roc curve from the Binary Classification Model</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "val rocCurve = metrics.asInstanceOf[BinaryClassificationMetricsModel].roc.map{ case ThresholdMetricModel(x, y) => (x,y)}", 
            "execution_count": 14, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "\n<p>Load the rocCurve into a dataframe</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "val rocDF = spark.createDataFrame(rocCurve).\n                    withColumnRenamed(\"_1\", \"FPR\").\n                    withColumnRenamed(\"_2\", \"TPR\")\nrocDF.show(3)", 
            "execution_count": 15, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+-------------------+------------------+\n|                FPR|               TPR|\n+-------------------+------------------+\n|                0.0|               0.0|\n|0.04216867469879518|0.8467741935483871|\n|                1.0|               1.0|\n+-------------------+------------------+\nonly showing top 3 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "println(metrics.getClass)\nprintln(rocDF.getClass)", 
            "execution_count": 16, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "class com.ibm.analytics.ngp.pipeline.evaluate.BinaryClassificationMetricsModel\nclass org.apache.spark.sql.Dataset\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "<p>Display the models ROC curve on the Brunel chart setting the \"False Positive Rate\" and \"True Positive Rate\"</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "scrolled": false
            }, 
            "source": "%%brunel data('rocDF') x(FPR) y(TPR) line tooltip(#all) axes(x:'False Positive Rate':grid, y:'True Positive Rate':grid) title('ROC') ", 
            "execution_count": 17, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/html": "\n         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/brunel.2.3.css\" charset=\"utf-8\">\n         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/sumoselect.css\" charset=\"utf-8\">\n         <style>  </style>\n         <div id=\"controlsId5a1b6285-6c8f-459f-84f0-de7dcb6056d9\" class=\"brunel\"/>\n<svg id=\"visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089\" width=\"500\" height=\"400\"></svg>\n\n<script>\nrequire.config({\n            waitSeconds: 60,\n            paths: {\n                'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n                'topojson' : '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n                'brunel' : 'https://brunelvis.org/js/brunel.2.3.min',\n                'brunelControls' : 'https://brunelvis.org/js/brunel.controls.2.3.min'\n            },\n\n            shim: {\n               'brunel' : {\n                    exports: 'BrunelD3',\n                    deps: ['d3', 'topojson'],\n                    init: function() {\n                       return {\n                         BrunelD3 : BrunelD3,\n                         BrunelData : BrunelData\n                      }\n                    }\n                },\n               'brunelControls' : {\n                    exports: 'BrunelEventHandlers',\n                    init: function() {\n                       return {\n                         BrunelEventHandlers: BrunelEventHandlers,\n                         BrunelJQueryControlFactory: BrunelJQueryControlFactory\n                      }\n                    }\n                }\n\n            }\n\n        });\n\n        require([\"d3\"], function(d3) {\n        require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n\n            function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 22, 43, 37, 13),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'move').call(zoom)\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    vis.append('clipPath').attr('id', 'clip_visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n    chart.append('text').attr('class', 'title header').text('ROC').style('text-anchor', 'middle')\n      .attr('x','50%')\n      .attr('y',2).attr('dy','0.8em');\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scaleLinear().domain([0, 1.0000001])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scaleLinear().domain([0, 1.0000001])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('False Positive Rate').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('True Positive Rate').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_width / 33.0)));\n    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_width / 20)));\n\n    function buildAxes(time) {\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n      BrunelD3.makeGrid(gridGroup, scale_x, geom.inner_height, true );\n      BrunelD3.makeGrid(gridGroup, scale_y, geom.inner_width, false );\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        scale_x = t.rescaleX(base_scales[0]);\n        scale_y = t.rescaleY(base_scales[1]);\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0)\n          .sortRows('FPR:ascending');\n        processed = post(processed, 0);\n        var f0 = processed.field('FPR'),\n          f1 = processed.field('TPR'),\n          f2 = processed.field('#row'),\n          f3 = processed.field('#selection');\n        var keyFunc = function(d) { return 'ALL' };\n        data = {\n          FPR:          function(d) { return f0.value(d.row) },\n          TPR:          function(d) { return f1.value(d.row) },\n          $row:         function(d) { return f2.value(d.row) },\n          $selection:   function(d) { return f3.value(d.row) },\n          FPR_f:        function(d) { return f0.valueFormatted(d.row) },\n          TPR_f:        function(d) { return f1.valueFormatted(d.row) },\n          $row_f:       function(d) { return f2.valueFormatted(d.row) },\n          $selection_f: function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return 'ALL' },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = Math.abs( scale_x(scale_x.domain()[0] + 0.04216867469879518) - scale_x.range()[0] );\n        var x = function(d) { return scale_x(data.FPR(d))};\n        var h = Math.abs( scale_y(scale_y.domain()[0] + 0.15322580645161288) - scale_y.range()[0] );\n        var y = function(d) { return scale_y(data.TPR(d))};\n        // Define paths\n        var path = d3.line().x(x).y(y);\n        var splits = BrunelD3.makePathSplits(data, path, x);\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element line')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .attr('d', function(d) { return d.path });\n        }\n\n        // Define labeling for the selection\n        function label(selection, transitionMillis) {\n\n          var tooltipLabeling  = {\n            index: -1, method: 'path', location: ['center', 'center'], inside: true, align: 'middle', pad: 0, dy: 0.3,\n            fit: true, granularity: 0,\n            path: path,\n            content: function(d) {\n              return d.row == null ? null : '<span class=\"title\">FPR: </span>'\n\t\t\t+ '<span class=\"field\">' + data.FPR_f(d) + '</span>'\n\t\t\t+ '<br/>'\n\t\t\t+ '<span class=\"title\">TPR: </span>'\n\t\t\t+ '<span class=\"field\">' + data.TPR_f(d) + '</span>'\n            }\n          };\n          BrunelD3.addTooltip(selection, tooltipLabeling, geom);\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(splits, function(d) { return d.key });\n        var added = selection.enter().append('path');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n        label(merged, transitionMillis);\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['FPR'],\n          y:            ['TPR']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['FPR', 'TPR'], \n   options: ['numeric', 'numeric'], \n   rows: [[0, 0], [0.0421687, 0.8467742], [1, 1], [1, 1]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v = new BrunelVis('visid8ea60039-aa0b-46c0-bfa0-70d41d6e8089');\nv .build(table1);\n\n            \"\"\n        });\n        });\n        </script>"
                    }, 
                    "metadata": {}, 
                    "execution_count": 17, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n## 3. DSX Local Machine Learning - Use Repository service to save model. ##\n-------------------------------------", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "\n### 3.1 Set credentials to the Watson Machine Learning Deployments ##\n-------------------------------------\n<p>val service_path = \"https://ibm-watson-ml.mybluemix.net\"<br>\nval instance_id = \"xxxx\"<br>\nval username = \"xxxx\"<br>\nval password = \"xxxx\"</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "// The code was removed by DSX for sharing.", 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "<p>Secure a connection to the repository and add author information for model</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "val ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)\n\nval model_artifact = MLRepositoryArtifact(model, trainingDF, \"Credit Card Churn Model\")\n\n//Add creater information for model\nval meta_with_author = model_artifact.meta.add(\"authorName\", \"DataScientist\");\nval mutableArtifact = MLRepositoryArtifact.mutableModelArtifact(model_artifact);\nval new_artifact = mutableArtifact.mutate(model, meta_with_author);\n\nval saved_model = ml_repository_client.models.save(new_artifact).get", 
            "execution_count": 19, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Sep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 18\nSep 21, 2017 5:58:57 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [labels, list, element] BINARY: 2 values, 23B raw, 24B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 564\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 268B for [labels, list, element] BINARY: 50 values, 314B raw, 237B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 120\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [numClasses] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [numFeatures] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [intercept] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [coefficients, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B for [coefficients, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [coefficients, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 113B for [coefficients, values, list, element] DOUBLE: 7 values, 69B raw, 70B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 308\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 116B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 12 values, 109B raw, 73B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 92\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 668\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 58B for [impurityStats, list, element] DOUBLE: 2 values, 15B raw, 17B comp, 1 pages, encodings: [PLAIN_DICTIONARY, RLE], dic { 1 entries, 8B raw, 1B comp}\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 93.52% for 12 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 86.32% for 13 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 80.16% for 14 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 74.81% for 15 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 230B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 42 values, 350B raw, 187B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 70.14% for 16 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 74.81% for 15 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 20B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 80.16% for 14 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 74.81% for 15 writers\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 80.16% for 14 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 86.32% for 13 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 80.16% for 14 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 74.81% for 15 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 70.14% for 16 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 74.81% for 15 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 70.14% for 16 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 60B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 19B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 74.81% for 15 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 70.14% for 16 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 74.81% for 15 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 80.16% for 14 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 86.32% for 13 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 93.52% for 12 writers\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 86.32% for 13 writers\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 93.52% for 12 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM WARNING: org.apache.parquet.hadoop.MemoryManager: Total allocation exceeds 95.00% (1,506,174,592 bytes) of heap memory\nScaling row group sizes to 93.52% for 12 writers\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 20B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 112\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 68B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 27B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 69B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 28B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [leftChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [rightChild] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, featureIndex] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 64B for [split, leftCategoriesOrThreshold, list, element] DOUBLE: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B for [split, numCategories] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [BIT_PACKED, PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 100\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 39B for [id] INT32: 1 values, 4B raw, 6B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [prediction] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [impurity] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B for [impurityStats, list, element] DOUBLE: 2 values, 29B raw, 20B comp, 1 pages, encodings: [PLAIN, RLE]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B for [gain] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [BIT_PACKED, PLAIN]\nSep 21, 2017 5:58:58 AM INFO: org.apache.parquet.had"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "<div class=\"alert alert-block alert-info\"> Note: If there is error about \"Failed to load class 'org.slf4j.impl.StaticLoggerBinder'\" in the cell above, that is expected.</div> ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "println(\"modelType: \" + saved_model.meta.prop(\"modelType\"))\nprintln(\"trainingDataSchema: \" + saved_model.meta.prop(\"trainingDataSchema\"))\nprintln(\"creationTime: \" + saved_model.meta.prop(\"creationTime\"))\nprintln(\"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\"))\nprintln(\"label: \" + saved_model.meta.prop(\"label\"))", 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "modelType: Some(sparkml-model-2.0)\ntrainingDataSchema: Some({\"type\":\"struct\",\"fields\":[{\"name\":\"AGE\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"ACTIVITY\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"EDUCATION\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SEX\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"STATE\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"NEGTWEETS\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"INCOME\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"label\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]})\ncreationTime: Some(2017-09-21T10:58:56.646Z)\nmodelVersionHref: Some(https://ibm-watson-ml.mybluemix.net/v2/artifacts/models/739058bb-1aec-4e8e-a148-f69ebb5860d4/versions/4c0e5dfb-7782-467b-ae02-e6eef69e1c3c)\nlabel: Some(label)\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "<div class=\"alert alert-block alert-info\"> Tip: modelVersionHref is our model unique indentifier in the Watson Machine Learning repository.</div> ", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n### 3.2 Load model and make predictions ##\n-------------------------------------", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "import play.api.libs.json._\nimport scalaj.http.{Http, HttpOptions}\n\nval model_version_href = saved_model.meta.prop(\"modelVersionHref\").get\nval loaded_model_artifact = ml_repository_client.models.version(model_version_href).get", 
            "execution_count": 21, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "loaded_model_artifact.name.mkString\n", 
            "execution_count": 22, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "Credit Card Churn Model"
                    }, 
                    "metadata": {}, 
                    "execution_count": 22, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "loaded_model_artifact match {\n        case SparkPipelineModelLoader(Success(model)) => {\n          val predictions = model.transform(validationDF)\n        }\n        case SparkPipelineModelLoader(Failure(e)) => \"Loading failed.\"\n        case _ => println(s\"Unexpected artifact class: ${loaded_model_artifact.getClass}\")\n    }\npredictions.select(\"AGE\",\"ACTIVITY\",\"EDUCATION\",\"SEX\",\"STATE\",\"NEGTWEETS\",\"INCOME\",\"label\").show()", 
            "execution_count": 23, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "+---+--------+---------+---+-----+---------+------+-----+\n|AGE|ACTIVITY|EDUCATION|SEX|STATE|NEGTWEETS|INCOME|label|\n+---+--------+---------+---+-----+---------+------+-----+\n| 20|       0|        1|  F|   ID|       13| 17877|  1.0|\n| 20|       1|        1|  M|   WV|        4| 20614|  0.0|\n| 20|       2|        1|  F|   KY|        3| 14811|  0.0|\n| 20|       2|        2|  M|   MI|       10| 43853|  1.0|\n| 20|       2|        4|  M|   PA|        5| 17511|  1.0|\n| 21|       0|        1|  F|   ND|       11| 20449|  1.0|\n| 21|       0|        4|  F|   MD|        8| 56976|  1.0|\n| 21|       0|        4|  M|   OR|       10| 71745|  1.0|\n| 21|       1|        1|  F|   ID|       10| 13852|  1.0|\n| 21|       1|        4|  F|   PA|       10| 45850|  1.0|\n| 21|       2|        1|  F|   MD|       10| 13390|  1.0|\n| 21|       2|        4|  M|   WA|        2| 22528|  0.0|\n| 21|       4|        1|  F|   DC|       10| 18064|  0.0|\n| 21|       4|        1|  F|   FL|        4| 18681|  0.0|\n| 21|       4|        1|  M|   OR|       11| 22262|  1.0|\n| 21|       4|        1|  M|   TX|        3| 22159|  0.0|\n| 21|       4|        4|  M|   NM|       10| 27159|  0.0|\n| 21|       5|        1|  F|   OK|        4| 29177|  0.0|\n| 22|       0|        1|  F|   PA|       12| 16228|  1.0|\n| 22|       1|        2|  F|   ID|        9| 70208|  1.0|\n+---+--------+---------+---+-----+---------+------+-----+\nonly showing top 20 rows\n\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "\n<p>Generate an access token to work with the Watson Machine Learning API</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "import java.util.Base64\nimport java.nio.charset.StandardCharsets\n\n// Get WML service instance token\nval wml_auth_header = \"Basic \" + Base64.getEncoder.encodeToString((username + \":\" + password).getBytes(StandardCharsets.UTF_8))\nval wml_url = service_path + \"/v3/identity/token\"\nval wml_response = Http(wml_url).header(\"Authorization\", wml_auth_header).asString\nval wmltoken_json: JsValue = Json.parse(wml_response.body)\n\nval wmltoken = (wmltoken_json \\ \"token\").asOpt[String] match {\n    case Some(x) => x\n    case None => \"\"\n}\nwmltoken", 
            "execution_count": 24, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "eyJhbGciOiJSUzUxMiIsInR5cCI6IkpXVCJ9.eyJ0ZW5hbnRJZCI6ImFjOTExNTMxLWYyYjQtNDQ4ZC04YTk3LWU2MjA0MWQ0NTFlZSIsImluc3RhbmNlSWQiOiJhYzkxMTUzMS1mMmI0LTQ0OGQtOGE5Ny1lNjIwNDFkNDUxZWUiLCJwbGFuSWQiOiIzZjZhY2Y0My1lZGU4LTQxM2EtYWM2OS1mOGFmM2JiMGNiZmUiLCJyZWdpb24iOiJ1cy1zb3V0aCIsInVzZXJJZCI6ImZjNjFjYzc4LTNjY2EtNGMyZi04NmFhLWRiNGI2ODEwODQzYyIsImlzcyI6Imh0dHA6Ly8xMjkuNDEuMjI5LjE4ODo4MDgwL3YyL2lkZW50aXR5IiwiaWF0IjoxNTA1OTkxNTg2LCJleHAiOjE1MDYwMjAzODZ9.OJyzaXrShE8MpuKNNIbT7sDdgeKqcYcx620aJ1rayIUeYtToRvEZTPbTpczNBprWrJvYzcuuCIdW6RUBl5wSm_powanUWSVYSqIjfnnK68sDPZpiwj_W1rpQWzAjrSE-hmo2SvvOX-rVYx8i_0oi24vjqqJQxI_AIiDih3Yt2zxKibKLvXAiKkz7XdHuxY7vyjWqrBHp3AIkbRKEkx9LN-1JcVTMQpUtuPxO8i1QnObsl4RSZN0aNGsxdQaOC_6EhonEIFmW-F5_X0QXqu_RAxeDHPu_9HkueJ-gwS1URzwolG3YysnjLRoV_KdIPSi_990__IHrj5LhJlee8QvfTQ"
                    }, 
                    "metadata": {}, 
                    "execution_count": 24, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n### 3.3 Get a WML response instance from Watson Machine Learning API ##\n-------------------------------------", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "val endpoint_instance = service_path + \"/v3/wml_instances/\" + instance_id\nval wml_response_instance = Http(endpoint_instance).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wmltoken).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString\nwml_response_instance\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "\n<p>Find the deployed Models and create an access url</p>", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "val published_models_json: JsValue = Json.parse(wml_response_instance.body)\nval published_models_url = (((published_models_json \\ \"entity\") \\\\ \"published_models\")(0) \\ \"url\").as[JsString].value\npublished_models_url\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "// Get a list of the published wml models.\nval wml_models = Http(published_models_url).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wmltoken).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString\n\nwml_models\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "var deployment_endpoint: String = _\nwml_models.body.split(\"\\\"\").map{ s => {if ((s contains \"deployments\") & (s contains saved_model.uid.mkString)) {deployment_endpoint = s}}}\n\ndeployment_endpoint\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n### 3.4 Create an Online Deployment for the Model ##\n-------------------------------------", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "// Create an online deployment for the published model.\nval payload_name = \"Online scoring\"\nval payload_data_online = Json.stringify(Json.toJson(Map(\"type\" -> \"online\", \"name\" -> payload_name)))\n\nprint (payload_data_online)", 
            "execution_count": 29, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "{\"type\":\"online\",\"name\":\"Online scoring\"}"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "val response_online = Http(deployment_endpoint).postData(payload_data_online).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wmltoken).option(HttpOptions.connTimeout(50000)).option(HttpOptions.readTimeout(50000)).asString\n\nprint (response_online)\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "val scoring_url_json: JsValue = Json.parse(response_online.body)\nprint (response_online.body)\nval scoring_url = (scoring_url_json \\ \"entity\" \\ \"scoring_url\").asOpt[String] match {\n    case Some(x) => x\n    case None => \"\"\n}\n\nscoring_url", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "\n\n\n\n## 4. Create online scoring endpoint ##\n-------------------------------------", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "// Create the playload_scoring json for the model.\nval payload_scoring = Json.stringify(Json.toJson(Map(\"fields\" -> Json.toJson(List(Json.toJson(\"AGE\"), Json.toJson(\"ACTIVITY\"), Json.toJson(\"EDUCATION\"), Json.toJson(\"SEX\"), Json.toJson(\"STATE\"), Json.toJson(\"NEGTWEETS\"), Json.toJson(\"INCOME\"), Json.toJson(\"label\"))),\n                                                    \"values\" -> Json.toJson(List(List(Json.toJson(41), Json.toJson(1), Json.toJson(4), Json.toJson(\"M\"), Json.toJson(\"TX\"), Json.toJson(4), Json.toJson(200000), Json.toJson(0)))))))\n", 
            "execution_count": 33, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "payload_scoring", 
            "execution_count": 34, 
            "cell_type": "code", 
            "outputs": [
                {
                    "data": {
                        "text/plain": "{\"fields\":[\"AGE\",\"ACTIVITY\",\"EDUCATION\",\"SEX\",\"STATE\",\"NEGTWEETS\",\"INCOME\",\"label\"],\"values\":[[41,1,4,\"M\",\"TX\",4,200000,0]]}"
                    }, 
                    "metadata": {}, 
                    "execution_count": 34, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "val response_scoring = Http(scoring_url).postData(payload_scoring).header(\"Content-Type\", \"application/json\").header(\"Authorization\", \"Bearer \" + wmltoken).option(HttpOptions.method(\"POST\")).option(HttpOptions.connTimeout(10000)).option(HttpOptions.readTimeout(50000)).asString\n\nprint (response_scoring)", 
            "execution_count": 35, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "HttpResponse({\n  \"fields\": [\"AGE\", \"ACTIVITY\", \"EDUCATION\", \"SEX\", \"STATE\", \"NEGTWEETS\", \"INCOME\", \"label\", \"gender_code\", \"state_code\", \"features\", \"rawPrediction\", \"probability\", \"prediction\"],\n  \"values\": [[41, 1, 4, \"M\", \"TX\", 4, 200000, 0.0, 0.0, 12.0, [41.0, 1.0, 4.0, 4.0, 200000.0, 0.0, 12.0], [1.664006626624147, -1.664006626624147], [0.8407751120977467, 0.15922488790225323], 0.0]]\n},200,Map(Cache-Control -> Vector(private, no-cache, no-store, must-revalidate), Connection -> Vector(Keep-Alive), Content-Type -> Vector(application/json), Date -> Vector(Thu, 21 Sep 2017 11:00:28 GMT), Pragma -> Vector(no-cache), Server -> Vector(nginx/1.11.5), Status -> Vector(HTTP/1.1 200 OK), Transfer-Encoding -> Vector(chunked), X-Backside-Transport -> Vector(OK OK), X-Content-Type-Options -> Vector(nosniff), X-Frame-Options -> Vector(DENY), X-Global-Transaction-ID -> Vector(3585359295), X-Xss-Protection -> Vector(1)))"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 1
}